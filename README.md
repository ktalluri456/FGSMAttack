# FGSM Attack Implementation

This project demonstrates how we can use the Fast Gradient Sign Method (FGSM) to destroy a neural network's ability to classify images, otherwise known as an adversarial attack.

## Tools Used

Tensorflow, Torch, Matplotlib, Numpy

## To Import

Import specific optimizers, normalizations, and neural network models necessary to implement both the neural network and the attack itself. 

## Usage

The way this can be used is to detect the strengths and weaknesses in the development of a classifier algorithm. The accuracy in this case reduces by 50%, so to optimize a neural network, we want to decrease the effectiveness of the adversarial attack. 